{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'contextual': True,\n",
    "    'timewindowsize': 10,\n",
    "    'smoothing_windowsize': 10,\n",
    "    'sparsing_coefficient': 24,\n",
    "    'labels_forward_shift': 0,\n",
    "    }\n",
    "\n",
    "modalities_list = ['audio', 'eyes', 'face_nn', 'kinect']\n",
    "features_num_list = [36, 6, 100, 27]\n",
    "frame_rate_list = [100, 50, 50, 15]\n",
    "\n",
    "X_raw = [None] * len(modalities_list)\n",
    "\n",
    "context = 6 # amount of context in seconds\n",
    "\n",
    "###############################################\n",
    "###  THIS IS WHAT YOU MIGHT WANT TO CHANGE  ###\n",
    "###############################################\n",
    "\n",
    "folder = '../' # Set the folder of test data \n",
    "save_prediction_folder = folder + 'submission_FedotovD/prediction/' # Set folder for prediction\n",
    "# The prediction files will NOT replace the original ones (with all zeros)\n",
    "\n",
    "print('Parameters are loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "num_sets = len(modalities_list)\n",
    "\n",
    "for mod in range(0,len(modalities_list)):\n",
    "    # Features\n",
    "    modality = modalities_list[mod]\n",
    "    features_num = features_num_list[mod]\n",
    "    \n",
    "    filenames_features = os.listdir(folder + modality)\n",
    "\n",
    "    names_list = ['F_' + str(elem) for elem in range(0,features_num)]\n",
    "\n",
    "    for num in range (0, len(filenames_features)):\n",
    "        temp = pd.read_csv(folder + modality + '/' + filenames_features[num], names=names_list, header=0)\n",
    "        if num == 0:\n",
    "            features = pd.DataFrame(temp.as_matrix(), \n",
    "                                    index=[filenames_features[num][:-4] + '_' + str(elem) for elem in np.round(temp.index.values, decimals=2)],\n",
    "                                    columns=temp.columns.values)\n",
    "        else:\n",
    "            features = features.append(pd.DataFrame(temp.as_matrix(),\n",
    "                                                    index=[filenames_features[num][:-4] + '_' + str(elem) for elem in np.round(temp.index.values, decimals=2)],\n",
    "                                                    columns=temp.columns.values))\n",
    "    X_raw[mod] = features\n",
    "    \n",
    "# Labels\n",
    "filenames_labels = os.listdir(folder + 'prediction')\n",
    "\n",
    "for num in range (0, len(filenames_labels)):\n",
    "    temp = pd.read_csv(folder + 'prediction/' + filenames_labels[num])\n",
    "    if num == 0:\n",
    "        labels = pd.DataFrame(temp.iloc[:,1:7].as_matrix(), \n",
    "                             index=[filenames_labels[num][:-4] + '_' + str(elem) for elem in round(temp.Time,2)],\n",
    "                             columns=temp.columns.values[1:7])\n",
    "    else:\n",
    "        labels = labels.append(pd.DataFrame(temp.iloc[:,1:7].as_matrix(),\n",
    "                                            index=[filenames_labels[num][:-4] + '_' + str(elem) for elem in round(temp.Time,2)],\n",
    "                                            columns=temp.columns.values[1:7]))\n",
    "        \n",
    "Y_raw = labels\n",
    "\n",
    "print('modality: (examples, features)\\n')\n",
    "for sets in range(num_sets):\n",
    "    print(modalities_list[sets] + ': ' + str(X_raw[sets].shape))\n",
    "print('labels: ' + str(Y_raw.shape))\n",
    "\n",
    "print('Data is loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "# Create index variable\n",
    "index_cut = [None] * num_sets\n",
    "for sets in range(num_sets):\n",
    "    index_cut[sets] = [itm for itm in X_raw[sets].index if itm in Y_raw.index]\n",
    "    \n",
    "def extract_partition_from_index (indices):\n",
    "    partition = int(indices[2:10], 16)\n",
    "    \n",
    "    return partition\n",
    "\n",
    "# This code performs the standard data preprocessing\n",
    "def data_normalizer(dataX, scalerX, imputer=True, features_norm=True, verbose=True):\n",
    "\n",
    "    # Create data frames for normalized data\n",
    "    dataX_norm = copy.deepcopy(dataX)\n",
    "    # Imput missing values (if any)\n",
    "    if (imputer):\n",
    "        dataX_norm.set_value(dataX_norm.index, dataX_norm.columns, np.nan_to_num(dataX_norm.as_matrix()))\n",
    "\n",
    "    # Normalize features\n",
    "    if (features_norm):                             \n",
    "        dataX_norm.set_value(dataX_norm.index, dataX_norm.columns, \n",
    "                                        scalerX.transform(dataX_norm.as_matrix()))\n",
    "    \n",
    "    return dataX_norm\n",
    "\n",
    "# This code transforms standard data representation (Samples x Features) into time-continuous (Samples x Features x Timesteps)\n",
    "# taking sparsing coefficient into account\n",
    "\n",
    "def smoother (prediction, index, index_cont, smoothing_windowsize, sc, labels_forward_shift):\n",
    "    prediction_smooth = np.zeros((len(index)))\n",
    "    for i in range(0,len(index)):\n",
    "        counter = 0\n",
    "        lower_bound = max(0,i - smoothing_windowsize*sc * max(1,labels_forward_shift))\n",
    "        upper_bound = i + smoothing_windowsize*sc * max(1,labels_forward_shift)\n",
    "        locate = np.where(index_cont[lower_bound:upper_bound] == index[i])\n",
    "        for loc_x in range(0, locate[0].shape[0]):\n",
    "            for loc_y in range(0, locate[1].shape[0]):\n",
    "                prediction_smooth[i] += prediction[lower_bound + locate[0][loc_x],locate[1][loc_y]]\n",
    "                counter += 1\n",
    "        prediction_smooth[i] /= counter \n",
    "\n",
    "            \n",
    "    return prediction_smooth\n",
    "\n",
    "# Filling arrays with time-continuous data\n",
    "def add_timesteps (X, Y, index, timewindowX, timewindowY, sc, labels_forward_shift):\n",
    "    \n",
    "    c = 1 - labels_forward_shift\n",
    "    # Create arrays for time-continuous data     \n",
    "    X_cont = np.zeros((X.shape[0], timewindowX, X.shape[1]))\n",
    "    Y_cont_index = np.array([([None] * timewindowY)] * Y.shape[0])\n",
    "    \n",
    "    # sc - sparsing coefficient\n",
    "    for i in range (X.shape[0]):\n",
    "        for j in range (0, timewindowX):\n",
    "            if (i - sc*timewindowX + sc*j + sc) >= 0:\n",
    "                if extract_partition_from_index(index[i]) == extract_partition_from_index(index[i - sc*timewindowX + sc*j + sc]): \n",
    "                    X_cont[i,j,:] = X[i - sc*timewindowX + sc*j + sc,:]\n",
    "\n",
    "        for j in range (0, timewindowY):\n",
    "            if 0 <= (i - sc*np.floor(timewindowY*c) + sc*j + sc*(0 ** (1 - c))) < Y.shape[0]: \n",
    "                if extract_partition_from_index(index[i]) == extract_partition_from_index(index[i - sc*int(np.floor(timewindowY*c)) + sc*j + sc*int(0 ** (1 - c))]):\n",
    "                    Y_cont_index[i,j] = index[i - sc*int(np.floor(timewindowY*c)) + sc*j + sc*int(0 ** (1 - c))]\n",
    "                    \n",
    "    return X_cont, Y_cont_index\n",
    "\n",
    "def preprocess_data (X_raw, Y_raw, scalerX_list, index_cut, parameters):\n",
    "    X = [None] * num_sets\n",
    "    X_ = [None] * num_sets\n",
    "    Y_cont_index = [None] * num_sets\n",
    "\n",
    "    print('Preprocessing...')\n",
    "    \n",
    "    for sets in range(num_sets):\n",
    "        print('Set: ' + str(sets))\n",
    "        X_[sets] = data_normalizer(X_raw[sets], scalerX_list[sets])\n",
    "\n",
    "        parameters['sparsing_coefficient'] = int(context * frame_rate_list[sets] / parameters['timewindowsize'])\n",
    "\n",
    "        [X[sets], Y_cont_index[sets]] = add_timesteps(X_[sets].loc[index_cut[sets]].as_matrix(),\n",
    "                                                      Y_raw.loc[index_cut[sets]].as_matrix(),\n",
    "                                                      index_cut[sets], parameters['timewindowsize'],\n",
    "                                                      parameters['smoothing_windowsize'], \n",
    "                                                      parameters['sparsing_coefficient'], \n",
    "                                                      parameters['labels_forward_shift'])\n",
    "\n",
    "    print('Preprocessing is completed!')\n",
    "    return X, Y_cont_index\n",
    "\n",
    "print('Preprocessing functions are defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib \n",
    "scalerX_list = [None] * len(modalities_list)\n",
    "for sets in range(0, len(modalities_list)):\n",
    "    scalerX_list[sets] = joblib.load('scalerX_' + modalities_list[sets] + '.pkl') \n",
    "[X, Y_cont_index] = preprocess_data (X_raw, Y_raw, scalerX_list, index_cut, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "print('Making predictions with pre-trained models...')\n",
    "prediction = [None] * num_sets\n",
    "\n",
    "for sets in range(num_sets):\n",
    "    print('\\nSet: ' + str(sets))\n",
    "    model = load_model('model_' + modalities_list[sets] + '.h5')\n",
    "    print('Model loaded!\\nPredicting...')\n",
    "    prediction[sets] = model.predict(X[sets], verbose=1)\n",
    "    \n",
    "print('\\nPredictions are made!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_smooth = [None] * num_sets\n",
    "prediction_DF = [None] * num_sets\n",
    "\n",
    "print('Smoothing prediction...\\nThere will be 4 sets, 6 parts each\\nIt takes time, be patient :)')\n",
    "\n",
    "for sets in range(num_sets):\n",
    "    prediction_smooth[sets] = np.zeros((prediction[sets].shape[0], prediction[sets].shape[2]))\n",
    "    print('Set: ' + str(sets))\n",
    "    for i in range(0,6):\n",
    "        print(str(i + 1) + '/6')\n",
    "        parameters['sparsing_coefficient'] = int(context * frame_rate_list[sets] / parameters['timewindowsize'])\n",
    "        prediction_smooth[sets][:,i] = smoother(prediction[sets][:,:,i], index_cut[sets], Y_cont_index[sets], parameters['smoothing_windowsize'], parameters['sparsing_coefficient'], parameters['labels_forward_shift'])\n",
    "\n",
    "    prediction_DF[sets] = pd.DataFrame(prediction_smooth[sets], index=index_cut[sets])\n",
    "\n",
    "for sets in range(num_sets):\n",
    "    print('Number of NaNs in prediction for ' + modalities_list[sets] + ': ' + str(np.sum(pd.isnull(prediction_DF[sets].any(axis=1)))))\n",
    "print('Predictions are smoothed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Multimodal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Fusing predictions...')\n",
    "\n",
    "# Interpolate\n",
    "pred_inter = [None] * len(modalities_list)\n",
    "for mod in range(0,len(modalities_list)):\n",
    "    pred_inter[mod] = pd.DataFrame(np.empty((Y_raw.shape[0],Y_raw.shape[1])) * np.nan, index=Y_raw.index, columns=prediction_DF[mod].columns)\n",
    "    pred_inter[mod].set_value(prediction_DF[mod].index, prediction_DF[mod].columns, prediction_DF[mod].as_matrix())\n",
    "    pred_inter[mod] = pred_inter[mod].interpolate()\n",
    "\n",
    "# Clear NaNs if any\n",
    "for mod in range(0,len(modalities_list)):\n",
    "    for i in range(0,pred_inter[mod].shape[0]):\n",
    "        if np.sum(np.isnan(pred_inter[mod].iloc[i,:].as_matrix())) == 6:\n",
    "            pred_inter[mod].set_value(pred_inter[mod].index.values[i], pred_inter[mod].columns, [0.18, 0.15, 0.08, 0.20, 0.20, 0.19])\n",
    "        else: break\n",
    "    print('Number of NaNs in ' + modalities_list[mod] + ': ' + str(np.sum(np.isnan(pred_inter[0].as_matrix()))/6))\n",
    "    \n",
    "# Adding data frames\n",
    "multimodal_results = pred_inter[0].add(pred_inter[1])\n",
    "multimodal_results2 = pred_inter[2].add(pred_inter[3])\n",
    "multimodal_results = multimodal_results.add(multimodal_results2)\n",
    "print('Predictions are fused!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Redo submission files for one label/tw\n",
    "\n",
    "print('Modifying predictions...')\n",
    "multimodal_results_new = copy.deepcopy(multimodal_results) \n",
    "from scipy import stats\n",
    "\n",
    "tw = 10000\n",
    "\n",
    "for num in range(0,len(filenames_labels)):\n",
    "    pred = multimodal_results.filter(like=filenames_labels[num][:10], axis=0)\n",
    "    temp = pred.as_matrix()\n",
    "\n",
    "    parts = int(np.ceil(temp.shape[0]/tw))\n",
    "    new_labels = np.zeros((temp.shape[0], temp.shape[1]))\n",
    "    for i in range (0,parts):\n",
    "        new_labels[range(i*tw,np.min([(i+1)*tw,temp.shape[0]])),\n",
    "                   stats.mode(np.argmax(temp[i*tw:np.min([(i+1)*tw,temp.shape[0]]),:],axis=1))[0][0]] = 1\n",
    "        \n",
    "    multimodal_results_new.loc[pred.index,:] = new_labels\n",
    "    \n",
    "multimodal_results_new.columns = ['Anger','Sad', 'Disgust', 'Happy', 'Scared', 'Neutral']\n",
    "multimodal_results_new.rename_axis('Time', axis=1)\n",
    "print('Predictions are modified!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Saving predictions...')\n",
    "for f in range(len(filenames_labels)):\n",
    "    multimodal_results_new.columns = ['Anger','Sad', 'Disgust', 'Happy', 'Scared', 'Neutral']\n",
    "    sub_file = multimodal_results_new.loc[[s for s in multimodal_results_new.index.values if filenames_labels[f][:10] in s]]\n",
    "    temp = pd.read_csv(folder + 'prediction/' + filenames_labels[f], engine='python')\n",
    "    sub_file.index = temp.Time\n",
    "    sub_file = sub_file.rename_axis('Time', axis=1)\n",
    "    \n",
    "    if not os.path.exists(save_prediction_folder):\n",
    "        os.makedirs(save_prediction_folder)\n",
    "    sub_file.to_csv(save_prediction_folder + filenames_labels[f])\n",
    "\n",
    "print('Predictions are saved!\\nThis was the last step!\\nThank you!')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
